{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5c498b-1344-4e9d-88d2-ab81e8975c24",
   "metadata": {},
   "source": [
    "# INTERACTIVE HUMAN IN THE LOOP STORYTELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acf928-5e72-4d54-9298-a788350de61f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786c4dda-ee84-4ebb-bc62-55e0a9ce8c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                storyid               storytitle  \\\n",
      "0  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd   David Drops the Weight   \n",
      "1  0beabab2-fb49-460e-a6e6-f35a202e3348              Frustration   \n",
      "2  87da1a22-df0b-410c-b186-439700b70ba6       Marcus Buys Khakis   \n",
      "3  2d16bcd6-692a-4fc0-8e7c-4a6f81d9efa9       Different Opinions   \n",
      "4  c71bb23b-7731-4233-8298-76ba6886cee1  Overcoming shortcomings   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0  David noticed he had put on a lot of weight re...   \n",
      "1                       Tom had a very short temper.   \n",
      "2  Marcus needed clothing for a business casual e...   \n",
      "3  Bobby thought Bill should buy a trailer and ha...   \n",
      "4          John was a pastor with a very bad memory.   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0  He examined his habits to try and figure out t...   \n",
      "1               One day a guest made him very angry.   \n",
      "2  All of his clothes were either too formal or t...   \n",
      "3  Bill thought a truck would be better for what ...   \n",
      "4  He tried to memorize his sermons many days in ...   \n",
      "\n",
      "                                           sentence3  \\\n",
      "0  He realized he'd been eating too much fast foo...   \n",
      "1        He punched a hole in the wall of his house.   \n",
      "2                He decided to buy a pair of khakis.   \n",
      "3  Bobby pointed out two vehicles were much more ...   \n",
      "4  He decided to learn to sing to overcome his ha...   \n",
      "\n",
      "                                           sentence4  \\\n",
      "0  He stopped going to burger places and started ...   \n",
      "1        Tom's guest became afraid and left quickly.   \n",
      "2              The pair he bought fit him perfectly.   \n",
      "3  Bill was set in his ways with conventional thi...   \n",
      "4  He then made all his sermons into music and sa...   \n",
      "\n",
      "                                           sentence5  \n",
      "0  After a few weeks, he started to feel much bet...  \n",
      "1  Tom sat on his couch filled with regret about ...  \n",
      "2  Marcus was happy to have the right clothes for...  \n",
      "3  He ended up buying the truck he wanted despite...  \n",
      "4      His congregation was delighted and so was he.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = 'datasets\\ROCStories_winter2017.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f2bab-2386-48cf-87e2-75ad2173b64b",
   "metadata": {},
   "source": [
    "## Segement Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0157afa7-2397-495a-8382-dd3ac83b715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925d04-8266-455f-acd2-a8c822552184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1ce964c-d44e-4eac-80da-5aeeef4ad492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing and cleaning stories...\n",
      "Generating embeddings...\n",
      "Building FAISS index...\n",
      "Saving preprocessed data...\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load ROCStories dataset.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# clean and normalize text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive whitespace\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\']', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "# tokenization and story processing\n",
    "def process_stories(df):\n",
    "    \"\"\"Process stories into a tokenized and cleaned format.\"\"\"\n",
    "    processed_stories = []\n",
    "    for _, row in df.iterrows():\n",
    "        story_id = row['storyid']\n",
    "        story_title = clean_text(row['storytitle'])\n",
    "        sentences = [clean_text(row[f'sentence{i}']) for i in range(1, 6)]\n",
    "        processed_stories.append({'StoryID': story_id, 'StoryTitle': story_title, 'Sentences': sentences})\n",
    "    return processed_stories\n",
    "\n",
    "# generate embeddings\n",
    "def generate_embeddings(processed_stories, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Generate sentence embeddings using Sentence Transformers.\"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    for story in processed_stories:\n",
    "        story_id = story['StoryID']\n",
    "        story_title = story['StoryTitle']\n",
    "        sentences = story['Sentences']\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            embedding = model.encode(sentence, convert_to_tensor=True).cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "            metadata.append({\n",
    "                'StoryID': story_id,\n",
    "                'StoryTitle': story_title,\n",
    "                'SentenceIndex': i,\n",
    "                'Sentence': sentence\n",
    "            })\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings, metadata\n",
    "\n",
    "# store embeddings in FAISS\n",
    "def build_faiss_index(embeddings):\n",
    "    \"\"\"Build and store FAISS index for retrieval.\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# save preprocessed data\n",
    "def save_preprocessed_data(metadata, index, index_file='faiss_index'):\n",
    "    \"\"\"Save metadata and FAISS index.\"\"\"\n",
    "    # save metadata\n",
    "    pd.DataFrame(metadata).to_csv('metadata.csv', index=False)\n",
    "    # save FAISS index\n",
    "    faiss.write_index(index, index_file)\n",
    "\n",
    "# main preprocessing pipeline\n",
    "def preprocess_pipeline(file_path):\n",
    "    \"\"\"Complete preprocessing pipeline for ROCStories dataset.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    print(\"Processing and cleaning stories...\")\n",
    "    processed_stories = process_stories(df)\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings, metadata = generate_embeddings(processed_stories)\n",
    "\n",
    "    print(\"Building FAISS index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    print(\"Saving preprocessed data...\")\n",
    "    save_preprocessed_data(metadata, index, index_file='faiss_index')  # pass FAISS index\n",
    "    print(\"Preprocessing complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'datasets\\ROCStories_winter2017.csv'\n",
    "    preprocess_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47aaa6c8-fade-46d0-8028-73c1c4a155b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoryID</th>\n",
       "      <th>StoryTitle</th>\n",
       "      <th>SentenceIndex</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd</td>\n",
       "      <td>David Drops the Weight</td>\n",
       "      <td>0</td>\n",
       "      <td>David noticed he had put on a lot of weight re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd</td>\n",
       "      <td>David Drops the Weight</td>\n",
       "      <td>1</td>\n",
       "      <td>He examined his habits to try and figure out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd</td>\n",
       "      <td>David Drops the Weight</td>\n",
       "      <td>2</td>\n",
       "      <td>He realized he'd been eating too much fast foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd</td>\n",
       "      <td>David Drops the Weight</td>\n",
       "      <td>3</td>\n",
       "      <td>He stopped going to burger places and started ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd</td>\n",
       "      <td>David Drops the Weight</td>\n",
       "      <td>4</td>\n",
       "      <td>After a few weeks, he started to feel much bet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                StoryID              StoryTitle  \\\n",
       "0  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  David Drops the Weight   \n",
       "1  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  David Drops the Weight   \n",
       "2  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  David Drops the Weight   \n",
       "3  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  David Drops the Weight   \n",
       "4  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  David Drops the Weight   \n",
       "\n",
       "   SentenceIndex                                           Sentence  \n",
       "0              0  David noticed he had put on a lot of weight re...  \n",
       "1              1  He examined his habits to try and figure out t...  \n",
       "2              2  He realized he'd been eating too much fast foo...  \n",
       "3              3  He stopped going to burger places and started ...  \n",
       "4              4  After a few weeks, he started to feel much bet...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = pd.read_csv('metadata.csv')\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af7e4186-8f32-4912-804c-2dc634d1bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('embeddings.npy', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b65c5a-2b0c-4666-8c75-525715d5f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the FAISS index\n",
    "index = faiss.read_index('faiss_index')\n",
    "\n",
    "# load metadata\n",
    "metadata = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "017a4023-a454-448d-9ed0-415a5c26d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# example query\n",
    "query = \"It's so good to be alive\"\n",
    "\n",
    "# generate query embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f4eded9-b495-4aea-a16d-b2df297295cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Sentence: Life felt so good, even though it didn't last long.\n",
      "Story Title: Living my dream\n",
      "Distance: 0.8068217635154724\n",
      "\n",
      "Result 2:\n",
      "Sentence: I began to wish I wasn't alive.\n",
      "Story Title: Not the Same\n",
      "Distance: 0.9622210264205933\n",
      "\n",
      "Result 3:\n",
      "Sentence: She felt so lucky and grateful to be alive\n",
      "Story Title: Chemo\n",
      "Distance: 1.0018548965454102\n",
      "\n",
      "Result 4:\n",
      "Sentence: They were luckily they made it alive.\n",
      "Story Title: Hot air balloon fire\n",
      "Distance: 1.0196583271026611\n",
      "\n",
      "Result 5:\n",
      "Sentence: His life was going great.\n",
      "Story Title: Best man needed\n",
      "Distance: 1.040922999382019\n",
      "\n",
      "Result 6:\n",
      "Sentence: I'm glad to be safely on the ground and for the trip to be over.\n",
      "Story Title: Airplane ride\n",
      "Distance: 1.0543153285980225\n",
      "\n",
      "Result 7:\n",
      "Sentence: I feel I am so very blessed in life.\n",
      "Story Title: Volunteering\n",
      "Distance: 1.0623960494995117\n",
      "\n",
      "Result 8:\n",
      "Sentence: It was alive so I went to save it too.\n",
      "Story Title: Trip The Cat\n",
      "Distance: 1.0711764097213745\n",
      "\n",
      "Result 9:\n",
      "Sentence: Luckily he was able to bring it to life.\n",
      "Story Title: Todd's Car Trouble\n",
      "Distance: 1.0745726823806763\n",
      "\n",
      "Result 10:\n",
      "Sentence: It felt so good\n",
      "Story Title: The Sun\n",
      "Distance: 1.076948642730713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of closest matches to retrieve\n",
    "top_k = 10\n",
    "\n",
    "# search\n",
    "distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Result {i + 1}:\")\n",
    "    print(f\"Sentence: {metadata.iloc[idx]['Sentence']}\")\n",
    "    print(f\"Story Title: {metadata.iloc[idx]['StoryTitle']}\")\n",
    "    print(f\"Distance: {distances[0][i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ed2fc29-3d5e-429b-82c3-9bdca2cd9fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m enhanced_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Generate output using GPT-4\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menhanced_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Display the generated story\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp2\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# user query\n",
    "user_query = \"Write a story about overcoming challenges.\"\n",
    "\n",
    "# retrieve similar sentences\n",
    "query_embedding = model.encode(user_query, convert_to_tensor=True).cpu().numpy()\n",
    "distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "# collect retrieved sentences\n",
    "retrieved_sentences = [metadata.iloc[idx]['Sentence'] for idx in indices[0]]\n",
    "\n",
    "# combine retrieved context with the query\n",
    "context = \" \".join(retrieved_sentences)\n",
    "enhanced_prompt = f\"{context}\\n\\nUser's query: {user_query}\"\n",
    "\n",
    "# generate output using GPT-4\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": enhanced_prompt}]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
